{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make labels using an LLM for each feature in a model\n",
    "For now, use gpt-4o-mini with a template derived from https://arxiv.org/pdf/2408.00657v1\n",
    "\n",
    "We want to provide maximum activating examples, as well as \"control\" samples that dont activate the feature. Instead of using random sample lets try to pull from similar features.\n",
    "TODO: double check my similarity search because the similar features don't seem to give similar samples at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from latentsae.sae import Sae\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 20020.54it/s]\n"
     ]
    }
   ],
   "source": [
    "sae = \"64_32\"\n",
    "model = Sae.load_from_hub(\"enjalot/sae-nomic-text-v1.5-FineWeb-edu-100BT\", sae)\n",
    "name = f\"NOMIC_FWEDU_{round(model.num_latents/1000)}k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = range(model.num_latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_df = pd.read_parquet(f\"data/top10_{sae}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [model.W_dec[i].detach().cpu().numpy() for i in range(model.W_dec.size(0))]\n",
    "def get_similar_features(feature):\n",
    "    # Get the weight vector for the given feature\n",
    "    feature_weight = weights[feature].reshape(1, -1)\n",
    "    # Calculate cosine similarity between the feature weight and all weights\n",
    "    similarities = cosine_similarity(feature_weight, weights)\n",
    "    sorted = np.argsort(similarities[0])[::-1]\n",
    "    # print(\"similarities\", sorted)\n",
    "    # print(\"similarities\", similarities[0][sorted])\n",
    "    # Get the indices of the top 5 most similar weights (excluding the feature itself)\n",
    "    similar_indices = np.argsort(similarities[0])[-6:-1][::-1]  # Get top 5, excluding the feature itself\n",
    "    return similar_indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_similar_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(feature):\n",
    "  # get top 10 activations\n",
    "  top = top10_df[top10_df.feature == feature][\"chunk_text\"].tolist()\n",
    "  # get 5 similar features\n",
    "  similar_features = get_similar_features(feature)\n",
    "  similar = top10_df[top10_df.feature.isin(similar_features)]\n",
    "  similar = similar[~similar.top_indices.isin([feature])]\n",
    "  \n",
    "  control = similar.sample(10)[\"chunk_text\"].tolist()\n",
    "  # grab 2 samples for each of those features\n",
    "  return { \"top\": top, \"control\": control, \"similar_features\": similar_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = get_samples(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f0[\"top\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f0[\"control\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f0[\"similar_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"You are a meticulous researcher conducting an important investigation into a certain\n",
    "neuron in a language model. Your task is to figure out what sort of behaviour this neuron is responsible for - namely, on what general concepts, features,\n",
    "themes, methodologies or topics does this neuron fire? Here's how you'll complete the task:\n",
    "\n",
    "INPUT DESCRIPTION: You will be given two inputs: 1) Max Activating Examples and 2)\n",
    "Zero Activating Examples.\n",
    "1. You will be given several examples of text that activate the neuron, along with a\n",
    "number being how much it was activated. This means there is some feature, theme,\n",
    "methodology, topic or concept in this text that 'excites' this neuron.\n",
    "2. You will also be given several examples of text that don't activate the neuron. This\n",
    "means the feature, topic or concept is not present in these texts.\n",
    "\n",
    "OUTPUT DESCRIPTION: Given the inputs provided, complete the following tasks.\n",
    "1. Based on the MAX ACTIVATING EXAMPLES provided, write down potential topics,\n",
    "concepts, themes, methodologies and features that they share in common. These\n",
    "will need to be specific - remember, all of the text comes from subject, so these\n",
    "need to be highly specific subject concepts. You may need to look at different\n",
    "levels of granularity (i.e. subsets of a more general topic). List as many as you can\n",
    "think of. Give higher weight to concepts more present/prominent in examples with\n",
    "higher activations.\n",
    "2. Based on the zero activating examples, rule out any of the topics/concepts/features\n",
    "listed above that are in the zero-activating examples. Systematically go through your\n",
    "list above.\n",
    "3. Based on the above two steps, perform a thorough analysis of which feature, concept\n",
    "or topic, at what level of granularity, is likely to activate this neuron. Use Occam's\n",
    "razor, as long as it fits the provided evidence. Be highly rational and analytical here.\n",
    "4. Based on step 4, summarise this concept in 1-8 words, in the form FINAL:\n",
    "<explanation>. Do NOT return anything after these 1-8 words.\n",
    "\n",
    "You should choose a label that best summarizes the theme of the list so that someone browsing the labels will have a good idea of what is in the list. \n",
    "Do not use punctuation, Do not explain yourself, respond with only a few words that summarize the list.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_prompt(feature):\n",
    "  samples = get_samples(feature)\n",
    "  prompt = f\"\"\"Here is a list of maximum activating samples for the neuron:\n",
    "  <Samples>\n",
    "  {\"\".join(f'<Sample>{sample}</Sample>' for sample in samples[\"top\"])}\n",
    "  </Samples>\n",
    "  Here is a list of zero activating samples:\n",
    "  <ControlSamples>\n",
    "  {\"\".join(f'<Sample>{sample}</Sample>' for sample in samples[\"control\"])}\n",
    "  </ControlSamples>\n",
    "  Please summarise this concept in 1-8 words, in the form FINAL:\n",
    "<explanation>. Do NOT return anything after these 1-8 words\n",
    "  \"\"\"\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 0\n",
    "messages=[\n",
    "    {\"role\":\"system\", \"content\": system_prompt}, \n",
    "    {\"role\":\"user\", \"content\": get_sample_prompt(feature)}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "async def chat(messages):\n",
    "  response = await client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    "  )\n",
    "  return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = await chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FINAL: properties and applications of gelatin'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 24576)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_parquet(f\"data/labels-{name}.parquet\")\n",
    "features_to_label = [feature for feature in features if feature not in labels_df['feature'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"features to label\", len(features_to_label), \"done with\", len(labels_df))\n",
    "# print(features_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features:   0%|          | 0/2438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features:  61%|██████    | 1476/2438 [1:01:36<28:37,  1.79s/it]Task exception was never retrieved\n",
      "future: <Task finished name='Task-2826' coro=<main() done, defined at /var/folders/sx/rrvr6l_d5x1_g46jxlx5ypfc0000gn/T/ipykernel_38579/3263936852.py:17> exception=KeyboardInterrupt()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/sx/rrvr6l_d5x1_g46jxlx5ypfc0000gn/T/ipykernel_38579/3263936852.py\", line 37, in <module>\n",
      "    asyncio.run(main(features))\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/nest_asyncio.py\", line 35, in run\n",
      "    loop.run_until_complete(task)\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 388, in __wakeup\n",
      "    self.__step(exc)\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 303, in __step\n",
      "    self.__step_run_and_handle_result(exc)\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 316, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sx/rrvr6l_d5x1_g46jxlx5ypfc0000gn/T/ipykernel_38579/3263936852.py\", line 21, in main\n",
      "    batch_results = await process_batch(batch)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sx/rrvr6l_d5x1_g46jxlx5ypfc0000gn/T/ipykernel_38579/3263936852.py\", line 15, in process_batch\n",
      "    return await asyncio.gather(*tasks)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
      "    future.result()\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 396, in __wakeup\n",
      "    self.__step()\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 303, in __step\n",
      "    self.__step_run_and_handle_result(exc)\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sx/rrvr6l_d5x1_g46jxlx5ypfc0000gn/T/ipykernel_38579/3263936852.py\", line 10, in chat_with_feature\n",
      "    response = await chat(messages)\n",
      "               ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sx/rrvr6l_d5x1_g46jxlx5ypfc0000gn/T/ipykernel_38579/3833420058.py\", line 3, in chat\n",
      "    response = await client.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1339, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1816, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1510, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1549, in _request\n",
      "    response = await self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/httpx/_client.py\", line 1661, in send\n",
      "    response = await self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/httpx/_client.py\", line 1689, in _send_handling_auth\n",
      "    response = await self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/httpx/_client.py\", line 1726, in _send_handling_redirects\n",
      "    response = await self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/httpx/_client.py\", line 1763, in _send_single_request\n",
      "    response = await transport.handle_async_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 373, in handle_async_request\n",
      "    resp = await self._pool.handle_async_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 216, in handle_async_request\n",
      "    raise exc from None\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 196, in handle_async_request\n",
      "    response = await connection.handle_async_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n",
      "    return await self._connection.handle_async_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/httpcore/_async/http11.py\", line 79, in handle_async_request\n",
      "    async with self._state_lock:\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/httpcore/_synchronization.py\", line 76, in __aenter__\n",
      "    await self._anyio_lock.acquire()\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/anyio/_core/_synchronization.py\", line 167, in acquire\n",
      "    await event.wait()\n",
      "  File \"/Users/enjalot/code/latent-taxonomy/venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 1662, in wait\n",
      "    await self._event.wait()\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/locks.py\", line 212, in wait\n",
      "    await fut\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/futures.py\", line 287, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Processing features: 100%|██████████| 2438/2438 [1:42:57<00:00,  2.53s/it]  \n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "limit = 1000\n",
    "batch_size = 10\n",
    "\n",
    "async def chat_with_feature(feature):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": get_sample_prompt(feature)}\n",
    "    ]\n",
    "    response = await chat(messages)\n",
    "    return {\"feature\": feature, \"label\": response}\n",
    "\n",
    "async def process_batch(features_batch):\n",
    "    tasks = [chat_with_feature(feature) for feature in features_batch]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "async def main(features, labels_df):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(features), batch_size), desc=\"Processing features\", leave=True):\n",
    "        batch = features[i:i + batch_size]\n",
    "        batch_results = await process_batch(batch)\n",
    "        results.extend(batch_results)\n",
    "        \n",
    "        # Save progress every 100 features\n",
    "        if len(results) % 100 == 0:\n",
    "            labels_df = pd.concat([labels_df, pd.DataFrame(results)])\n",
    "            labels_df.to_parquet(f\"data/labels-{name}.parquet\")\n",
    "            results = []\n",
    "        \n",
    "        # Respect rate limit\n",
    "        if len(results) % limit == 0:\n",
    "            await asyncio.sleep(60)  # Wait for a minute to respect rate limit\n",
    "\n",
    "    # Save any remaining results\n",
    "    labels_df = pd.concat([labels_df, pd.DataFrame(results)])\n",
    "    labels_df.to_parquet(f\"data/labels-{name}.parquet\")\n",
    "\n",
    "asyncio.run(main(features_to_label, labels_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
